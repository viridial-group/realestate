# SystÃ¨me de Scraping - Agences ImmobiliÃ¨res

Ce systÃ¨me permet de collecter automatiquement les informations des agences immobiliÃ¨res et des sociÃ©tÃ©s du domaine immobilier depuis Internet et de les sauvegarder dans la base de donnÃ©es PostgreSQL.

## ğŸš€ Installation

### PrÃ©requis

- Node.js 18+ et npm
- PostgreSQL avec la base de donnÃ©es `realestate_db`
- AccÃ¨s Internet

### Installation des dÃ©pendances

```bash
npm install
```

### Configuration

1. Copiez le fichier `.env.example` vers `.env`:
```bash
cp .env.example .env
```

2. Modifiez le fichier `.env` avec vos paramÃ¨tres de base de donnÃ©es:
```env
DB_HOST=localhost
DB_PORT=5432
DB_NAME=realestate_db
DB_USER=postgres
DB_PASSWORD=your_password
```

## ğŸ“‹ Utilisation

### Compilation TypeScript

```bash
npm run build
```

### ExÃ©cution

```bash
# Mode production (compilÃ©)
npm start

# Mode dÃ©veloppement (avec ts-node)
npm run dev
```

### Script combinÃ©

```bash
npm run scrape
```

## ğŸ—ï¸ Architecture

```
scraping/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/          # Configuration
â”‚   â”œâ”€â”€ database/        # Connexion et opÃ©rations DB
â”‚   â”œâ”€â”€ models/          # ModÃ¨les de donnÃ©es
â”‚   â”œâ”€â”€ scrapers/        # Scrapers pour diffÃ©rents sites
â”‚   â”œâ”€â”€ services/        # Services mÃ©tier
â”‚   â””â”€â”€ utils/           # Utilitaires (logger, validators)
â”œâ”€â”€ dist/                # Code compilÃ©
â”œâ”€â”€ logs/                # Fichiers de logs
â””â”€â”€ package.json
```

## ğŸ” Scrapers Disponibles

1. **GoogleScraper** - Recherche Google pour trouver des agences
2. **PagesJaunesScraper** - PagesJaunes.fr pour les informations dÃ©taillÃ©es
3. **LeboncoinScraper** - Leboncoin.fr pour les annonces d'agences

## ğŸ“Š DonnÃ©es CollectÃ©es

Pour chaque agence, le systÃ¨me collecte :
- Nom de l'agence
- Description
- Email
- TÃ©lÃ©phone
- Adresse complÃ¨te
- Ville
- Code postal
- Pays
- Site web
- Domaine
- Logo (URL)
- Source de la donnÃ©e

## ğŸ—„ï¸ Base de DonnÃ©es

Les donnÃ©es sont sauvegardÃ©es dans la table `organizations` avec les champs suivants :
- `name` - Nom de l'organisation
- `description` - Description
- `email` - Email
- `phone` - TÃ©lÃ©phone
- `address` - Adresse
- `city` - Ville
- `postal_code` - Code postal
- `country` - Pays
- `domain` - Domaine du site web
- `logo_url` - URL du logo
- `active` - Statut actif/inactif

## âš™ï¸ Configuration AvancÃ©e

### ParamÃ¨tres de scraping

Dans le fichier `.env` :
- `SCRAPING_DELAY` - DÃ©lai entre les requÃªtes (ms)
- `SCRAPING_TIMEOUT` - Timeout des requÃªtes (ms)
- `MAX_RETRIES` - Nombre de tentatives en cas d'Ã©chec
- `CONCURRENT_REQUESTS` - Nombre de requÃªtes simultanÃ©es

### Termes de recherche

Modifiez `SEARCH_TERMS` et `CITIES` dans `.env` pour personnaliser les recherches.

## ğŸ“ Logs

Les logs sont sauvegardÃ©s dans le dossier `logs/` :
- `scraping.log` - Logs gÃ©nÃ©raux
- `error.log` - Erreurs uniquement

## ğŸ”’ SÃ©curitÃ© et Ã‰thique

- Respect des `robots.txt`
- DÃ©lais entre les requÃªtes pour ne pas surcharger les serveurs
- Rotation des User-Agents
- Gestion des erreurs et retry automatique

## ğŸš§ AmÃ©liorations Futures

- [ ] Support de Puppeteer pour les sites JavaScript lourds
- [ ] Scraping de sites supplÃ©mentaires (SeLoger, PAP, etc.)
- [ ] Extraction d'emails depuis les sites web
- [ ] GÃ©ocodage automatique des adresses
- [ ] DÃ©tection de doublons amÃ©liorÃ©e
- [ ] Interface web pour monitorer le scraping
- [ ] Export des donnÃ©es en CSV/JSON

## ğŸ“„ Licence

ISC

